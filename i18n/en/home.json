{
    "Layout": {
        "home": "Home",
        "careers": "Careers",
        "about": "About",
        "product": "KaiBot",
        "research": "Research",
        "email": "Email",
        "subscribe": "Subscribe",
        "subscribed": "Subscribed",
        "footerRightTit": "Subscribe Kinetix AI",
        "footerRightP": "Get the latest information on Kinetix AI in real-time",
        "footerInputTxt": "your email address",
        "footerCopyright": "Copyright © 2025 - 2026. Kinetix AI 超维动力. All rights reserved.",
        "footerAddress": "Shenzhen Bay Eco-Technology Park, Nanshan District, Shenzhen, Guangdong Province, China"

    },
    "Home": {
        "title": "Kinetix AI",
        "indexBannerTit": "Building Ultra-human-like Robot<p>AGI Emergent Capability for Intelligence, Motion, and Feeling</p>",
        "indexCompanyTit": "<span>Vision</span>",
        "indexCompanyPara": "<p>At Kinetix AI, we're a team of passionate tech enthusiasts building robots that feel and think like humans. Our breakthrough, Physical World Model for Whole-body Control system, gives the humanoid a unique blend of wise decision-making and genuine emotion. By testing our robots in everyday life, we're creating a future where intelligent machines naturally integrate into every part of your world. Join us on this exciting journey as we redefine what's possible with technology.</p>",
        "indexCompanyBtn": "Learn more",
        "indexTechTit": "Core <span>Essentials</span>",
        "indexTechSTit": "From the 'Five Core Technologies' to' Silicon based Life '",
        "indexSceneTit": "Core <span>scenario</span>",
        "indexNewsTit": "<span>News</span> Center",
        "indexNewsBtn": "Learn More"
    },
    "Careers": {
        "allpositions": "All positions",
        "filterCriteria": "Filter criteria",
        "emptyTxt": "No suitable positions found !",
        "departmentsTit": "Department",
        "locationsTit": "Location",
        "workTypesTit": "Work type",
        "allDepartments": "All Department",
        "allLocations": "All Location",
        "allWorkTypes": "All Work type",
        "loading": "Loading"
    },
    "Company": {
        "bannerTit": "About",
        "introTit": "Company",
        "introPara": "<p>At Kinetix AI, we're a team of passionate tech enthusiasts building robots that feel and think like humans. Our breakthrough, Physical World Model for Whole-body Control system, gives the humanoid a unique blend of wise decision-making and genuine emotion. By testing our robots in everyday life, we're creating a future where intelligent machines naturally integrate into every part of your world. Join us on this exciting journey as we redefine what's possible with technology.</p><p>Our core team is laser-focused on robotics, autonomous driving, and AI. With top-notch engineering skills, we turn groundbreaking ideas into real-world products, shaping the future of physical AI agents. We've brought together a dozen world-class AI scientists whose work has been cited more than a few hundred thousand times on Google Scholar—proof of our global leadership in algorithm innovation.</p>​",
        "teamTit": "Our Stories"
    },
    "Product": {
        "txt": "Coming in early 2026. Stay tuned."
    },
    "Research": {
        "label_en": "Research",
        "para_en": "<p>Dive into the tech and engineering that make embodied intelligence actually work in the real world. Built for autonomous Ultra-Human-Like Robots.</p>",
        "label": "研究",
        "para": "<p>探索具身智能落地所需的每一个技术和工程细节</p><p>真正打造超拟人人形机器人</p>"
    },
    "ResearchDetail1": {
        "label": "Research",
        "titleTips": "AMS",
        "title": "统一人形机器人全身控制框架：首次实现在单一策略中同时具备动态运动跟踪和极限平衡控制能力",
        "share1": "arXiv",
        "share2": "YouTube",
        "share3": "Code (Est. Feb 2026) ",
        "copy": "Share",
        "date": "November 24, 2025",
        "copysuccess": "页面链接已复制",
        "paraTitle1": "极端平衡动作",
        "paraTitle2": "动态动作",
        "paraTitle3": "基于惯性测量单元（IMU）的遥操作",
        "paraTitle4": "基于RGB相机的实时遥操作",
        "paraTitle5": "摘要",
        "paraTitle6": "AMS概述",
        "paraTitle7": "引用",
        "para1Subtitle1": "叶问蹲 (未见数据的零样本泛化)",
        "para1Subtitle2": "单腿平衡站立(动作为随机生成，旨在挑战机器人极限难度下的能力)",
        "para2ItemTitle1": "奔跑",
        "para2ItemTitle2": "篮球运球",
        "para2ItemTitle3": "腰部动作(未见视频数据的零样本泛化)",
        "para2ItemTitle4": "弓步动作 (未见视频数据的零样本泛化)",
        "para2ItemTitle5": "摆动与深蹲 (未见视频数据的零样本泛化)",
        "para2ItemTitle6": "祈祷式下蹲 (未见视频数据的零样本泛化)",
        "para3Subtitle1": "(未使用动作捕捉系统, 1x)",
        "para3Item1Title1": "中国功夫",
        "para3Item1Title2": "踢球",
        "para3Subtitle2": "多机器人遥操作",
        "para5Artical": "<p>人形机器人被设想在以人为中心的环境中执行广泛的任务，这要求机器人既能具备敏捷性，又能保持稳健的平衡能力。近来在运动控制与全身跟踪方面的进展，使得机器人在“敏捷的动态技能”或“对稳定性要求极高的行为”这两类能力上分别取得了令人印象深刻的进步，但现有方法仍然偏向专用化，很难兼顾，大多只能专注于其中一种能力，并牺牲另一种能力。</p><p>在本工作中，我们提出 AMS（Agility Meets Stability，敏捷与稳定融合）——首个在单一策略（single policy）中同时统一动态动作跟踪与极限平衡保持的框架。我们的核心观点是利用异构数据源：一类是提供丰富、敏捷行为的人体动作捕捉数据集；另一类是受物理约束的合成平衡动作，用于刻画稳定性相关的姿态/配置。为协调“敏捷”与“稳定”这两种分歧明显的优化目标，我们还设计了一种混合奖励机制：在所有数据上施加通用的跟踪目标，同时仅在合成动作上注入与平衡相关的先验。在以上研究的基础上，我们提出一种自适应学习策略，结合基于性能驱动的采样与针对不同动作的奖励塑形，使得在多样化动作分布上能够实现高效训练。</p><p>我们在仿真环境以及Unitree G1 人形机器人上对 AMS 进行了全面验证（仿真测试、真机测试）。实验结果表明，单一策略情况下，搭载AMS框架的机器人不仅能够执行如跳舞、跑步等敏捷技能，还能以 zero-shot（零样本）方式完成诸如“叶问蹲（Ip Man’s Squat）”这类极限平衡动作，这表明 AMS 作为一种面向未来人形机器人应用的通用型控制范式具有极大潜力。</p>",
        "para6Artical": "<p>(a)通用的全身动作跟踪流程会将人类 MoCap（动作捕捉）数据重定向为参考动作，并采用一种基于教学关系（teacher-student）的强化学习策略。为应对数据限制以及相互冲突的优化目标，AMS 引入了如下三个关键组件。 (b) 生成的合成平衡数据，将作为人类 MoCap（动作捕捉）数据的补充部分，解决数据源不足的问题。 (c) 采用自学习的方式：基于每个单独动作的表现，进行自适应采样与奖励塑造（reward shaping）。 (d) 混合奖励机制（Hybrid rewards）被设计为，对所有动作发放通用奖励，同时只针对合成动作引入平衡先验奖励。</p>"
    },
    "ResearchDetail2": {
        "label": "Research",
        "titleTips": "WholeBodyVLA",
        "title": "一种面向人形机器人移动操作的全身VLA框架",
        "share1": "Video",
        "share2": "arXiv",
        "share4": "GitHub",
        "date": "December 16, 2025",
        "paraTitle1": "WholeBodyVLA 概述",
        "paraTitle2": "模型概述",
        "paraTitle3": "任务 1: 打包包装袋",
        "paraTitle4": "任务 2: 搬运纸箱",
        "paraTitle5": "任务 3: 推车前行",
        "paraTitle6": "泛化实验",
        "paraTitle7": "导航能力",
        "paraTitle8": "超长程双手协同作业",
        "paraTitle9": "更多尝试",
        "para1Artical": "WholeBodyVLA是一套在 Agibot X2 机器人上部署运行的人形机器人控制系统，它首次在大空间范围内实现了端到端的人形机器人“移动-操作”协同。基于该系统，Agibot X2能够自主完成连续任务，包括：(a-c) 基本的双臂抓取、侧步走向箱子并蹲下放置；(d-e) 蹲下抓取并抬起箱子，然后转身将箱子放到推车上；(f-h) 抓住推车把手，向前推车，并推动超过 50 公斤的负载。",
        "para2Artical": "<span>WholeBodyVLA作业流程图 </span> WholeBodyVLA采用了一个创新的双层架构设计，实现人形机器人的全身运动控制。整个系统的工作流程可以分为预训练阶段和实时运行阶段。  预训练阶段，潜在动作模型(LAM，Latent Action Model)通过学习纯操作视频和具有操作感知移动视频等两类视频数据，为VLM（视觉-语言模型)提供统一的潜在动作监督信号，并建立起从视觉观察到动作意图的映射关系。与此同时，系统还训练了一个基于强化学习的LMO（移动操作控制）策略，专门负责在各种干扰条件下实现精确、稳定的移动控制，从而确保机器人在执行复杂任务时能够兼顾平衡和稳定性。  实时运行阶段，VLM模块实时获取第一视角(egocentric)摄像头捕获的图像和用户输入的语言指令，并将多模态信息编码为潜在动作标记(latent action tokens)，随后以约10Hz的频率把Tokens解码为两个并行的控制信号：第一个是双臂关节动作指令，用于控制机器人的精密操作；第二个是移动命令，以50赫兹的高频率传输给运动控制模块，并实现稳健的全身协同操作。",
        "para3Subtitle1": "我们的成功案例",
        "para3Subtitle2": "基线模型的失败案例",
        "para3Item1Title1": "WholeBodyVLA (我们的)",
        "para3Item1Title2": "WholeBodyVLA在移动视野下的表现",
        "para3Item2Title1": "❌ 绊停",
        "para3Item2Title2": "❌ 失去平衡 误踢纸箱",
        "para4Subtitle1": "我们的成功案例",
        "para4Subtitle2": "基线模型的失败案例",
        "para4Item1Title1": "WholeBodyVLA (我们的)",
        "para4Item1Title2": "WholeBodyVLA在（未泛化）目标物体时的表现",
        "para4Item2Title1": "❌ 绊停",
        "para4Item2Title2": "❌ 失去平衡，偏离指定方向",
        "para5Subtitle1": "我们的成功案例",
        "para5Subtitle2": "基线模型的失败案例",
        "para5Item1Title1": "WholeBodyVLA (我们的)",
        "para5Item1Title2": "WholeBodyVLA在（未泛化）重负荷运载物下的表现",
        "para5Item2Title1": "❌ 偏离正确方向",
        "para5Item2Title2": "❌ 过晚停止",
        "para6Subtitle1": "1. 在目标物体不同状态下的泛化能力",
        "para6Subtitle2": "2. 在不同初始姿势下的泛化能力",
        "para6Subtitle3": "3. 在不同地形下的泛化能力",
        "para6SubArtical1": "展示WholeBodyVLA对不同物体外观、位置变化、布局变化以及颜色的鲁棒性。",
        "para6SubArtical2": "展示WholeBodyVLA在面对多样化起始姿势（X/Y偏移量、方向和桌面高度）时的能力，包括前移、侧移、转向和下蹲的系列动作执行。",
        "para6SubArtical3": "展示WholeBodyVLA穿越崎岖地形的性能。",
        "para6Subtitle1s": "X轴距离变化",
        "para6Subtitle2s": "Y轴距离变化",
        "para6Subtitle3s": "方位变化",
        "para6Subtitle4s": "高度变化",
        "para6Item1Title1": "X轴距离泛化实验1",
        "para6Item1Title2": "X轴距离泛化实验2 (WholeBodyVLA在未经泛化桌面颜色情况下的表现)",
        "para6Item2Title1": "Y轴距离泛化实验1",
        "para6Item2Title2": "Y轴距离泛化实验1 (WholeBodyVLA在未经泛化桌面颜色情况下的表现)",
        "para6Item3Title1": "方位变化泛化实验1",
        "para6Item3Title2": "方位变化泛化实验2 (WholeBodyVLA在未经泛化桌面颜色情况下的表现)",
        "para6Item4Title1": "高度泛化实验",
        "para7Artical": "展示WholeBodyVLA在长轨迹任务中的视觉导航和避障能力。",
        "para7ItemTitle1": "视觉导航，标识追踪",
        "para7ItemTitle2": "规避障碍物",
        "para8Artical": "展示WholeBodyVLA在肢体操控及全身协调动作上的长时序作业能力。",
        "para8ItemTitle1": "长程作业下的双臂协作",
        "para9Artical": "展示WholeBodyVLA在更复杂的日常操作任务（如擦拭、操作吸尘器等）中的可扩展性."
    }
}
