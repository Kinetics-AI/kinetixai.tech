{
    "Layout": {
        "home": "Home",
        "careers": "Careers",
        "about": "About",
        "product": "KaiBot",
        "research": "Research",
        "email": "Email",
        "subscribe": "Subscribe",
        "subscribed": "Subscribed",
        "footerRightTit": "Subscribe Kinetix AI",
        "footerRightP": "Get the latest information on Kinetix AI in real-time",
        "footerInputTxt": "your email address",
        "footerCopyright": "Copyright © 2025 - 2026. Kinetix AI 超维动力. All rights reserved.",
        "footerAddress": "Shenzhen Bay Eco-Technology Park, Nanshan District, Shenzhen, Guangdong Province, China"

    },
    "Home": {
        "title": "Kinetix AI",
        "indexBannerTit": "Building Ultra-human-like Robot<p>AGI Emergent Capability for Intelligence, Motion, and Feeling</p>",
        "indexCompanyTit": "<span>Vision</span>",
        "indexCompanyPara": "<p>At Kinetix AI, we're a team of passionate tech enthusiasts building robots that feel and think like humans. Our breakthrough, Physical World Model for Whole-body Control system, gives the humanoid a unique blend of wise decision-making and genuine emotion. By testing our robots in everyday life, we're creating a future where intelligent machines naturally integrate into every part of your world. Join us on this exciting journey as we redefine what's possible with technology.</p>",
        "indexCompanyBtn": "Learn more",
        "indexTechTit": "Core <span>Essentials</span>",
        "indexTechSTit": "From the 'Five Core Technologies' to' Silicon based Life '",
        "indexSceneTit": "Core <span>scenario</span>",
        "indexNewsTit": "<span>News</span> Center",
        "indexNewsBtn": "Learn More"
    },
    "Careers": {
        "allpositions": "All positions",
        "filterCriteria": "Filter criteria",
        "emptyTxt": "No suitable positions found !",
        "departmentsTit": "Department",
        "locationsTit": "Location",
        "workTypesTit": "Work type",
        "allDepartments": "All Department",
        "allLocations": "All Location",
        "allWorkTypes": "All Work type",
        "loading": "Loading"
    },
    "Company": {
        "bannerTit": "About",
        "introTit": "Company",
        "introPara": "<p>At Kinetix AI, we're a team of passionate tech enthusiasts building robots that feel and think like humans. Our breakthrough, Physical World Model for Whole-body Control system, gives the humanoid a unique blend of wise decision-making and genuine emotion. By testing our robots in everyday life, we're creating a future where intelligent machines naturally integrate into every part of your world. Join us on this exciting journey as we redefine what's possible with technology.</p><p>Our core team is laser-focused on robotics, autonomous driving, and AI. With top-notch engineering skills, we turn groundbreaking ideas into real-world products, shaping the future of physical AI agents. We've brought together a dozen world-class AI scientists whose work has been cited more than a few hundred thousand times on Google Scholar—proof of our global leadership in algorithm innovation.</p>​",
        "teamTit": "Our Stories"
    },
    "Product": {
        "txt": "Coming in early 2026. Stay tuned."
    },
    "Research": {
        "label_en": "Research",
        "para_en": "<p>Dive into the tech and engineering that make embodied intelligence actually work in the real world. Built for autonomous Ultra-Human-Like Robots.</p>",
        "label": "研究",
        "para": "<p>探索具身智能落地所需的每一个技术和工程细节</p><p>真正打造超拟人人形机器人</p>"
    },
    "ResearchDetail1": {
        "label": "Research",
        "titleTips": "AMS",
        "title": "统一人形机器人全身控制框架：首次实现在单一策略中同时具备动态运动跟踪和极限平衡控制能力",
        "share1": "arXiv",
        "share2": "YouTube",
        "share3": "Code (Est. Feb 2026) ",
        "copy": "Share",
        "date": "November 24, 2025",
        "copysuccess": "页面链接已复制",
        "paraTitle1": "极端平衡动作",
        "paraTitle2": "动态动作",
        "paraTitle3": "基于惯性测量单元（IMU）的遥操作",
        "paraTitle4": "基于RGB相机的实时遥操作",
        "paraTitle5": "摘要",
        "paraTitle6": "AMS概述",
        "paraTitle7": "引用",
        "para1Subtitle1": "叶问蹲 (未见数据的零样本泛化)",
        "para1Subtitle2": "单腿平衡站立(动作为随机生成，旨在挑战机器人极限难度下的能力)",
        "para2ItemTitle1": "奔跑",
        "para2ItemTitle2": "篮球运球",
        "para2ItemTitle3": "腰部动作(未见视频数据的零样本泛化)",
        "para2ItemTitle4": "弓步动作 (未见视频数据的零样本泛化)",
        "para2ItemTitle5": "摆动与深蹲 (未见视频数据的零样本泛化)",
        "para2ItemTitle6": "祈祷式下蹲 (未见视频数据的零样本泛化)",
        "para3Subtitle1": "(未使用动作捕捉系统, 1x)",
        "para3Item1Title1": "中国功夫",
        "para3Item1Title2": "踢球",
        "para3Subtitle2": "多机器人遥操作",
        "para5Artical": "<p>人形机器人被设想在以人为中心的环境中执行广泛的任务，这要求机器人既能具备敏捷性，又能保持稳健的平衡能力。近来在运动控制与全身跟踪方面的进展，使得机器人在“敏捷的动态技能”或“对稳定性要求极高的行为”这两类能力上分别取得了令人印象深刻的进步，但现有方法仍然偏向专用化，很难兼顾，大多只能专注于其中一种能力，并牺牲另一种能力。</p><p>在本工作中，我们提出 AMS（Agility Meets Stability，敏捷与稳定融合）——首个在单一策略（single policy）中同时统一动态动作跟踪与极限平衡保持的框架。我们的核心观点是利用异构数据源：一类是提供丰富、敏捷行为的人体动作捕捉数据集；另一类是受物理约束的合成平衡动作，用于刻画稳定性相关的姿态/配置。为协调“敏捷”与“稳定”这两种分歧明显的优化目标，我们还设计了一种混合奖励机制：在所有数据上施加通用的跟踪目标，同时仅在合成动作上注入与平衡相关的先验。在以上研究的基础上，我们提出一种自适应学习策略，结合基于性能驱动的采样与针对不同动作的奖励塑形，使得在多样化动作分布上能够实现高效训练。</p><p>我们在仿真环境以及Unitree G1 人形机器人上对 AMS 进行了全面验证（仿真测试、真机测试）。实验结果表明，单一策略情况下，搭载AMS框架的机器人不仅能够执行如跳舞、跑步等敏捷技能，还能以 zero-shot（零样本）方式完成诸如“叶问蹲（Ip Man’s Squat）”这类极限平衡动作，这表明 AMS 作为一种面向未来人形机器人应用的通用型控制范式具有极大潜力。</p>",
        "para6Artical": "<p>(a)通用的全身动作跟踪流程会将人类 MoCap（动作捕捉）数据重定向为参考动作，并采用一种基于教学关系（teacher-student）的强化学习策略。为应对数据限制以及相互冲突的优化目标，AMS 引入了如下三个关键组件。 (b) 生成的合成平衡数据，将作为人类 MoCap（动作捕捉）数据的补充部分，解决数据源不足的问题。 (c) 采用自学习的方式：基于每个单独动作的表现，进行自适应采样与奖励塑造（reward shaping）。 (d) 混合奖励机制（Hybrid rewards）被设计为，对所有动作发放通用奖励，同时只针对合成动作引入平衡先验奖励。</p>"
    },
    "ResearchDetail2": {
        "label": "Research",
        "titleTips": "WholeBodyVLA",
        "title": "一种面向人形机器人移动操作的全身VLA框架",
        "share1": "Video",
        "share2": "arXiv",
        "share4": "GitHub",
        "date": "December 16, 2025",
        "paraTitle1": "WholeBodyVLA 概述",
        "paraTitle2": "模型概述",
        "paraTitle3": "任务 1: 打包包装袋",
        "paraTitle4": "任务 2: 搬运纸箱",
        "paraTitle5": "任务 3: 推车前行",
        "paraTitle6": "泛化实验",
        "paraTitle7": "导航能力",
        "paraTitle8": "超长程双手协同作业",
        "paraTitle9": "更多尝试",
        "para1Artical": "WholeBodyVLA是一套在 Agibot X2 机器人上部署运行的人形机器人控制系统，它首次在大空间范围内实现了端到端的人形机器人“移动-操作”协同。基于该系统，Agibot X2能够自主完成连续任务，包括：(a-c) 基本的双臂抓取、侧步走向箱子并蹲下放置；(d-e) 蹲下抓取并抬起箱子，然后转身将箱子放到推车上；(f-h) 抓住推车把手，向前推车，并推动超过 50 公斤的负载。",
        "para2Artical": "<span>WholeBodyVLA作业流程图 </span> WholeBodyVLA采用了一个创新的双层架构设计，实现人形机器人的全身运动控制。整个系统的工作流程可以分为预训练阶段和实时运行阶段。  预训练阶段，潜在动作模型(LAM，Latent Action Model)通过学习纯操作视频和具有操作感知移动视频等两类视频数据，为VLM（视觉-语言模型)提供统一的潜在动作监督信号，并建立起从视觉观察到动作意图的映射关系。与此同时，系统还训练了一个基于强化学习的LMO（移动操作控制）策略，专门负责在各种干扰条件下实现精确、稳定的移动控制，从而确保机器人在执行复杂任务时能够兼顾平衡和稳定性。  实时运行阶段，VLM模块实时获取第一视角(egocentric)摄像头捕获的图像和用户输入的语言指令，并将多模态信息编码为潜在动作标记(latent action tokens)，随后以约10Hz的频率把Tokens解码为两个并行的控制信号：第一个是双臂关节动作指令，用于控制机器人的精密操作；第二个是移动命令，以50赫兹的高频率传输给运动控制模块，并实现稳健的全身协同操作。",
        "para3Subtitle1": "我们的成功案例",
        "para3Subtitle2": "基线模型的失败案例",
        "para3Item1Title1": "WholeBodyVLA (我们的)",
        "para3Item1Title2": "WholeBodyVLA在移动视野下的表现",
        "para3Item2Title1": "❌ 绊停",
        "para3Item2Title2": "❌ 失去平衡 误踢纸箱",
        "para4Subtitle1": "我们的成功案例",
        "para4Subtitle2": "基线模型的失败案例",
        "para4Item1Title1": "WholeBodyVLA (我们的)",
        "para4Item1Title2": "WholeBodyVLA在（未泛化）目标物体时的表现",
        "para4Item2Title1": "❌ 绊停",
        "para4Item2Title2": "❌ 失去平衡，偏离指定方向",
        "para5Subtitle1": "我们的成功案例",
        "para5Subtitle2": "基线模型的失败案例",
        "para5Item1Title1": "WholeBodyVLA (我们的)",
        "para5Item1Title2": "WholeBodyVLA在（未泛化）重负荷运载物下的表现",
        "para5Item2Title1": "❌ 偏离正确方向",
        "para5Item2Title2": "❌ 过晚停止",
        "para6Subtitle1": "1. 在目标物体不同状态下的泛化能力",
        "para6Subtitle2": "2. 在不同初始姿势下的泛化能力",
        "para6Subtitle3": "3. 在不同地形下的泛化能力",
        "para6SubArtical1": "展示WholeBodyVLA对不同物体外观、位置变化、布局变化以及颜色的鲁棒性。",
        "para6SubArtical2": "展示WholeBodyVLA在面对多样化起始姿势（X/Y偏移量、方向和桌面高度）时的能力，包括前移、侧移、转向和下蹲的系列动作执行。",
        "para6SubArtical3": "展示WholeBodyVLA穿越崎岖地形的性能。",
        "para6Subtitle1s": "X轴距离变化",
        "para6Subtitle2s": "Y轴距离变化",
        "para6Subtitle3s": "方位变化",
        "para6Subtitle4s": "高度变化",
        "para6Item1Title1": "X轴距离泛化实验1",
        "para6Item1Title2": "X轴距离泛化实验2 (WholeBodyVLA在未经泛化桌面颜色情况下的表现)",
        "para6Item2Title1": "Y轴距离泛化实验1",
        "para6Item2Title2": "Y轴距离泛化实验1 (WholeBodyVLA在未经泛化桌面颜色情况下的表现)",
        "para6Item3Title1": "方位变化泛化实验1",
        "para6Item3Title2": "方位变化泛化实验2 (WholeBodyVLA在未经泛化桌面颜色情况下的表现)",
        "para6Item4Title1": "高度泛化实验",
        "para7Artical": "展示WholeBodyVLA在长轨迹任务中的视觉导航和避障能力。",
        "para7ItemTitle1": "视觉导航，标识追踪",
        "para7ItemTitle2": "规避障碍物",
        "para8Artical": "展示WholeBodyVLA在肢体操控及全身协调动作上的长时序作业能力。",
        "para8ItemTitle1": "长程作业下的双臂协作",
        "para9Artical": "展示WholeBodyVLA在更复杂的日常操作任务（如擦拭、操作吸尘器等）中的可扩展性."
    },
    "ResearchDetail3": {
        "label": "Research",
        "title": "χ0: A Live-Stream Robotic Teamwork for Clothing Manipulation from Zero to Hero",
        "paraTitle1": "Veni, vidi, vici.",
        "paraTitle2": "Methodology",
        "paraTitle3": "Mode Consistency: Addressing the Distributional Trilemma",
        "paraTitle4": "Model Arithmetic",
        "paraTitle5": "Stage Advantage",
        "paraTitle6": "Bottom Line",
        "paraTitle7": "Citation",
        "para1Label": "Julius Caesar",
        "para1Artical1": "\"Veni, Vidi, Vici\" - I came, I saw, I conquered. Just as Julius Caesar's swift victory at Zela defined strategic efficiency, we aim to conquer the \"Mount Everest\" of robotics: 100% reliability in real-world garment manipulation. In this mission, Veni (data collection), Vidi (model training), and Vici (real-robot inference) form a progressive, interlocking chain where every link is critical. While foundational models like Pi01 and GO-12 rely on \"brute-force\" scale to reach these heights, we present a resource-aware alternative. We demonstrate how to take a system from 0% to 100% reliability using a fraction of the standard cost—specifically, within 20 hours of human demonstration and 8xA100 GPUs, rather than the tens of thousands of hours and hundreds of GPUs typically required.",
        "para1Artical2": "We achieve this through a three-stage technical philosophy that optimizes the transition from seeing to conquering:",
        "para1Artical3": "<ul><li><i>Mode Consistency:</i>  We argue that not all data is equally valuable. By strictly aligning the human demonstration distribution (<i>P</i><sub>train</sub>), the model's knowledge (<i>Q</i><sub>model</sub>), and the test-time distribution (<i>P</i><sub>test</sub>), we minimize in-distribution shifts that lead to failure.</li><li><i>Model Arithmetic:</i>  We move beyond the search for a single perfect checkpoint. We introduce a weight-space shortcut to merge models trained on different data manifolds into a single, adaptive policy-capturing diverse knowledge without the architectural complexity of MoE.</li><li><i>Stage Advantage:</i>  To conquer the \"last mile\", we decompose tasks into semantic stages. By estimating a stage-aware advantage signal, we provide the model with \"long-horizon vision,\" ensuring every action makes measurable progress toward the goal.</li></ul>",
        "para1Artical4": "This recipe provides strong evidence that 100% robotic skill mastery is a matter of strategic alignment, not just massive scale.",
        "para1Artical5": "We will release data, checkpoints, and host Challenge in 2026.",
        "para1Artical6": "Three tasks varying from folding to hanging, each covering a 4-hour duration, presented in 100x time-lapse format with critical segments highlighted at 2-5x speed.",
        "para1Artical7": "<span>Mode Consistency System Architecture-Left:</span> Human expert demonstration collection. Middle: Mixing models from different data sources via Model Arithmetic. Right: Real-robot inference. Bottom: DAgger Feedback and Stage Advantage from on-policy experience.",
        "para3Artical1": "We define the solution space for a given task as <i>P</i><sub>real</sub>, a distribution encompassing all valid actions that successfully accomplish the task. Achieving robust manipulation can be conceptualized as solving a dynamic alignment problem-akin to a game of \"Whac-a-Mole\" - among three distinct distributions:",
        "para3Artical2": "<ul><li><i>P</i><sub>train</sub> (Human Demonstrations): The empirical distribution collected from the human expert policy, <i>Q</i><sub>human</sub>. This represents a limited subset of the valid solution manifold <i>P</i><sub>real</sub>.</li><li><i>Q</i><sub>model</sub> (Learned Policy): The policy distribution parameterized by the model weights.</li><li><i>P</i><sub>test</sub> (Real-World Execution): The actual distribution of actions executed during physical deployment, inferred from <i>Q</i><sub>model</sub>. Success is defined by the intersection of <i>P</i><sub>test</sub> and <i>P</i><sub>real</sub>, while the portion of Ptest falling outside <i>P</i><sub>real</sub> constitutes the failure modes.</li></ul>",
        "para3Artical3": "Standard imitation learning paradigms generally aim to minimize the divergence KL(<i>Q</i><sub>model</sub>|<i>Q</i><sub>human</sub>) over the support of <i>P</i><sub>train</sub><sup>3,4</sup>. This process yields a finetuned <i>Q</i><sub>model</sub>, deploying this model via an inference function (denoted as Inference) results in the realized distribution <i>P</i><sub>test</sub>.",
        "para3Artical4": "Distribution dynamics of <i>P</i><sub>train</sub>, <i>Q</i><sub>model</sub>, and <i>P</i><sub>test</sub>.",
        "para3Artical5": "However, we identify underlying inconsistencies within this standard process:",
        "para3Artical6": "<ul><li>Distribution Shift between <i>P</i><sub>train</sub> and <i>P</i><sub>test</sub>: Interpreted as covariate shift, where the model encounters states during deployment that were absent in the training data.</li><li>Deployment Discrepancy between <i>Q</i><sub>model</sub> and <i>P</i><sub>test</sub>: Arising from the Inference function, where the model output is distorted during control, leading to unexpected failures.</li></ul>",
        "para3Artical7": "Back to basics, we propose two fundamental strategies-operating on Data Scope and Inference Scope - to stabilize these distributions:",
        "para3Artical8": "<ul><li>Data Scope:<ul><li>DAgger (Iterative Correction):  Static offline demonstrations often lack exposure to failure modes that inevitably emerge during real-world deployment<sup>5,6,7</sup>. We inject on-policy recovery trajectories - utilizing both heuristic methods and iterative DAgger - to populate these sparse regions. These \"corrected\" trajectories expand <i>P</i><sub>train</sub> towards underrepresented but critical regions of <i>P</i><sub>real</sub>.</li><li>Spatio-Temporal Augmentation:  To further bridge the gap between <i>P</i><sub>train</sub> and <i>P</i><sub>test</sub>, we employ structured augmentations across space and time. Spatially, we utilize mirroring and symmetry to enhance the model's understanding of dual-arm coordination. Temporally, time-scaling introduces variability in trajectory speed, allowing the model to adapt to fluctuating control frequencies.</li></ul></li></ul>",
        "para3Artical9": "<ul><li></li><li>Inference Scope:  To optimize the translation of <i>Q</i><sub>model</sub> into <i>P</i><sub>test</sub> via the Inference function, we utilize Chunk-wise Temporal Smoothing coupled with Real-time Chunking.<sup>8</sup> This effectively minimizes execution jitter and ensures the policy's intended actions are translated flawlessly into smooth, coherent real-robot execution. And we found that such smoothness contributes much to the final performance, i.e., success rate.</li></ul>",
        "para3ItemTitle1": "<span>DAgger</span>-Injecting on-policy recovery trajectories to expand <i>P</i><sub>train</sub> towards underrepresented failure modes in <i>P</i><sub>real</sub>.",
        "para3ItemTitle2": "Inference Optimization-Minimizing execution jitter to ensure smooth translation from <i>Q</i><sub>model</sub> to <i>P</i><sub>test</sub>.",
        "para3Artical10": "To visualize how our methods achieve progressive consistency among <i>P</i><sub>train</sub>, <i>Q</i><sub>model</sub>, and <i>P</i><sub>test</sub>, we project their action distributions on TaskA into a 3D t-SNE space.",
        "para3Artical11": "Interactive 3D t-SNE visualization of action distributions for <i>P</i><sub>train</sub>, <i>Q</i><sub>model</sub>, and <i>P</i><sub>test</sub>. Click and drag to rotate the plot.",
        "para3Artical12": "We posit that mode consistency constitutes a fundamental prerequisite for a model's core capabilities. On-policy experience and control optimization emerge as the most critical factors in achieving this alignment at the foundational stage.",
        "para4Artical1": "<i>How can we efficiently advance through iterative research cycles in embodied AI?</i> Unlike traditional CV or NLP, robotic manipulation tasks-especially complex ones like garment manipulation-demand extensive real-world data collection. This creates a persistent dilemma: as we iteratively gather new data, how do we know it meaningfully improves the model, and when is it enough? Retraining from scratch with the full dataset after each collection cycle is prohibitively expensive.",
        "para4Artical2": "<i>We propose Model Arithmetic (MA):</i> train only on newly collected data subsets, then merge the resulting model with previous ones through weight interpolation<sup>9</sup> guided by on-policy optimization. Remarkably, this synthesis can even surpass the performance of models trained on the entire dataset at once.",
        "para4Artical3": "We merge models trained on different data subsets into a single entity using weight interpolation, with the mixing weights optimized against on-policy data.",
        "para4Artical4": "<i>But how does MA work?</i> Our key insight: models trained on different data subsets actually learn distinct manifolds. MA acts as a shortcut to merge modes from these diverse manifolds, ensuring mode consistency in the final model. This approach soaks up capabilities from each iteration without costly full retraining.",
        "para4Artical5": "The merged model surpasses both the best constituent models and the oracle model trained on the full dataset across multiple tasks, evidencing that Model Arithmetic successfully assimilates the distinct policy manifolds learned from diverse data subsets.",
        "para5Artical1": "Long-horizon manipulation is inherently hindered by a simple question: <i>given multiple plausible actions at the same state, which one actually makes progress?</i> It is illustrated in cases where visual differences are subtle but result differences are huge given different actions. This is indeed why an advantage signal is needed.<sup>10</sup>",
        "para5Artical2": "Prior approach<sup>11</sup> obtains advantage implicitly by scoring the current state and the post-action state, then taking their difference. This formulation treats advantage as the subtraction of two independently estimated progress values, although advantage is inherently a relational quantity that depends on how an action transforms one state into another.",
        "para5Artical3": "We take a more straightforward route by treating advantage as a direct modeling target, predicting relative improvement from paired observations rather than deriving it from value predictions. This recasts advantage estimation to a single prediction problem, avoiding error compounding and yielding a smoother, more reliable state-to-state supervision signal.",
        "para5Artical4": "Comparison of cumulative progress induced by different methods along an inference-time manipulation trajectory. Green and red segments indicate higher- and lower-ranked actions based on predicted advantage, reflecting relative preference for task advancement. <span>Direct+Stage (ours)</span> produces smoother and more consistent progress accumulation than Value-diff.",
        "para5Artical5": "Built on this, the Stage Advantage decomposes long-horizon manipulation into a sequence of semantic stages, each corresponding to a meaningful sub-goal in the task. Instead of evaluating actions against the full task horizon, it estimates whether each action is likely to advance the current stage, providing a stage-aware advantage signal for policy training.",
        "para5Artical6": "The impact of these designs is evaluated using a combination of task success rate and the quality of the advantage signal, measured in terms of temporal smoothness and stability, as reported in the following evaluation.",
        "para5Artical7": "<span>Value-diff</span> computes the advantage by subtracting two independently predicted state values. <span>Direct</span> predicts the advantage as the relative improvement from paired observations. <span>Direct+Stage (ours)</span> uses stage-conditioned direct advantage prediction for long-horizon training, achieving smoother results (lower MSTD), greater stability (higher SFR), and higher success rates.",
        "para6Artical": "<ul><li>Not all robot data is equally valuable. The quality and characteristics of training data significantly impact policy performance.</li><li>The capability of the base policy is crucial, but knowing how to rapidly evaluate and select a capable base policy is even more important. A fast research iteration cycle is essential for developing intuition and understanding of base policy behavior.</li><li>Model arithmetic can surprisingly turn lead into gold. Simple arithmetic operations on model weights can transform mediocre policies into high-performing ones.</li><li>Stage-conditioned advantage estimation still has room for improvement. Revisiting fundamental concepts from reinforcement learning may unlock further gains.</li></ul>"
    }
}
