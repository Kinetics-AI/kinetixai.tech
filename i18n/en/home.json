{
    "Layout": {
        "home": "Home",
        "careers": "Careers",
        "about": "About",
        "product": "KaiBot",
        "research": "Research",
        "email": "Email",
        "subscribe": "Subscribe",
        "subscribed": "Subscribed",
        "footerRightTit": "Subscribe Kinetix AI",
        "footerRightP": "Get the latest information on Kinetix AI in real-time",
        "footerInputTxt": "your email address",
        "footerCopyright": "Copyright © 2025. Kinetix All rights reserved."

    },
    "Home": {
        "title": "Kinetix AI",
        "indexBannerTit": "Building Ultra-human-like Robot<p>AGI Emergent Capability for Intelligence, Motion, and Feeling</p>",
        "indexCompanyTit": "<span>Vision</span>",
        "indexCompanyPara": "<p>At Kinetix AI, we're a team of passionate tech enthusiasts building robots that feel and think like humans. Our breakthrough, Physical World Model for Whole-body Control system, gives the humanoid a unique blend of wise decision-making and genuine emotion. By testing our robots in everyday life, we're creating a future where intelligent machines naturally integrate into every part of your world. Join us on this exciting journey as we redefine what's possible with technology.</p>",
        "indexCompanyBtn": "Learn more",
        "indexTechTit": "Core <span>Essentials</span>",
        "indexTechSTit": "From the 'Five Core Technologies' to' Silicon based Life '",
        "indexSceneTit": "Core <span>scenario</span>",
        "indexNewsTit": "<span>News</span> Center",
        "indexNewsBtn": "Learn More"
    },
    "Careers": {
        "allpositions": "All positions",
        "filterCriteria": "Filter criteria",
        "emptyTxt": "No suitable positions found !",
        "departmentsTit": "Department",
        "locationsTit": "Location",
        "workTypesTit": "Work type",
        "allDepartments": "All Department",
        "allLocations": "All Location",
        "allWorkTypes": "All Work type",
        "loading": "Loading"
    },
    "Company": {
        "bannerTit": "About",
        "introTit": "Company",
        "introPara": "<p>At Kinetix AI, we're a team of passionate tech enthusiasts building robots that feel and think like humans. Our breakthrough, Physical World Model for Whole-body Control system, gives the humanoid a unique blend of wise decision-making and genuine emotion. By testing our robots in everyday life, we're creating a future where intelligent machines naturally integrate into every part of your world. Join us on this exciting journey as we redefine what's possible with technology.</p><p>Our core team is laser-focused on robotics, autonomous driving, and AI. With top-notch engineering skills, we turn groundbreaking ideas into real-world products, shaping the future of physical AI agents. We've brought together a dozen world-class AI scientists whose work has been cited more than a few hundred thousand times on Google Scholar—proof of our global leadership in algorithm innovation.</p>​",
        "teamTit": "Our Stories"
    },
    "Product": {
        "txt": "Coming in early 2026. Stay tuned."
    },
    "Research": {
        "label": "Research",
        "para": "<p>Dive into the tech and engineering that make embodied intelligence actually work in the real world. Built for autonomous Ultra-Human-Like Robots.</p>"
    },
    "ResearchDetail1": {
        "label": "Research",
        "titleTips": "AMS",
        "title": "Agility Meets Stability: Versatile Humanoid Control with Heterogeneous Data",
        "share1": "arXiv",
        "share2": "YouTube",
        "share3": "Code (Est. Feb 2026) ",
        "copy": "Share",
        "date": "November 24, 2025",
        "copysuccess": "The page link has been copied",
        "paraTitle1": "Extreme Balance Motions",
        "paraTitle2": "Dynamic Motions",
        "paraTitle3": "IMU-based Teleoperation",
        "paraTitle4": "RGB-based Real-time Teleoperation",
        "paraTitle5": "Abstract",
        "paraTitle6": "Overview of AMS",
        "paraTitle7": "Citation",
        "para1Subtitle1": "Ip Man's Squat (Unseen Data) - (叶问蹲)",
        "para1Subtitle2": "Single-Leg Balance Standing (Randomly Generated)",
        "para2ItemTitle1": "Running",
        "para2ItemTitle2": "Basketball Dribbling",
        "para2ItemTitle3": "Waist Twists (Unseen Data from Video)",
        "para2ItemTitle4": "Lunge (Unseen Data from Video)",
        "para2ItemTitle5": "Swing and Squat (Unseen Data from Video)",
        "para2ItemTitle6": "Prayer Squat (Unseen Data from Video)",
        "para3Subtitle1": "(No Optical MoCap System in Use, 1x)",
        "para3Item1Title1": "Kungfu",
        "para3Item1Title2": "Football",
        "para3Subtitle2": "Multi-robots Teleoperation",
        "para5Artical": "<p>Humanoid robots are envisioned to perform a wide range of tasks in human-centered environments, requiring controllers that combine agility with robust balance. Recent advances in locomotion and whole-body tracking have enabled impressive progress in either agile dynamic skills or stability-critical behaviors, but existing methods remain specialized, focusing on one capability while compromising the other.</p><p>In this work, we introduce AMS (Agility Meets Stability), the first framework that unifies both dynamic motion tracking and extreme balance maintenance in a single policy. Our key insight is to leverage heterogeneous data sources: human motion capture datasets that provide rich, agile behaviors, and physically constrained synthetic balance motions that capture stability configurations. To reconcile the divergent optimization goals of agility and stability, we design a hybrid reward scheme that applies general tracking objectives across all data while injecting balance-specific priors only into synthetic motions. Further, an adaptive learning strategy with performance-driven sampling and motion-specific reward shaping enables efficient training across diverse motion distributions.</p><p>We validate AMS extensively in simulation and on a real Unitree G1 humanoid. Experiments demonstrate that a single policy can execute agile skills such as dancing and running, while also performing zero-shot extreme balance motions like Ip Man's Squat, highlighting AMS as a versatile control paradigm for future humanoid applications.</p>",
        "para6Artical": "<p>(a) The general whole-body tracking pipeline retargets human MoCap data to reference motions and adopts a teacher-student-based strategy for reinforcement learning To address data limitations and conflicting optimization objectives, AMS introduces three key components as follows. (b) Synthetic balance data is generated to complement human MoCap data and address data limitations. (c) Adaptive learning is employed with adaptive sampling and reward shaping based on individual motion performance. (d) Hybrid rewards are designed with general rewards for all motions and balance prior rewards exclusively for synthetic motions.</p>"
    },
    "ResearchDetail2": {
        "label": "Research",
        "titleTips": "WholeBodyVLA",
        "title": "Towards Unified Latent VLA for Whole-body Loco-manipulation Control",
        "share1": "Video",
        "share2": "arXiv",
        "share3": "Paper",
        "share4": "GitHub",
        "date": "November 24, 2025",
        "paraTitle1": "Overview of WholeBodyVLA",
        "paraTitle2": "Method Overview",
        "paraTitle3": "Task 1: Bag Packing",
        "paraTitle4": "Task 2: Box Loading",
        "paraTitle5": "Task 3: Cart Pushing",
        "paraTitle6": "Generalization Experiments",
        "paraTitle7": "Navigation Capabilities",
        "paraTitle8": "Long-Horizon Bimanual Manipulation",
        "paraTitle9": "What's More",
        "para1Artical": "Introducing WholeBodyVLA, a humanoid system that operates on Agibot X2 robot and performs end-to-end humanoid loco–manipulation in large space for the first time. The proposed system achieves consecutive tasks autonomously, including (a-c) basic bimanual grasping, side-step toward the box, and squatting to place; (d-e) squatting to grasp and lift the box and turning to place the box onto the cart; (f-h) grasping the cart handle, pushing the cart forward, and pushing a load of more than 50 kg.",
        "para2Artical": "<span>Pipeline of WholeBodyVLA.</span> LAM is pretrained on manipulation and manipulation- aware locomotion videos, yielding unified latent supervision for the VLM. Meanwhile, the LMO RL policy is trained for precise and stable locomotion under disturbances. At runtime, egocentric images and language instructions are encoded by the VLM into latent action tokens, which are decoded (∼10 Hz) into (i) dual-arm joint actions and (ii) locomotion commands executed by LMO at 50 Hz, enabling robust whole-body loco–manipulation.",
        "para3Subtitle1": "Our Success Cases",
        "para3Subtitle2": "Failure Cases of Baseline Methods",
        "para3Item1Title1": "WholeBodyVLA (ours)",
        "para3Item1Title2": "WholeBodyVLA under visual variation",
        "para3Item2Title1": "❌ Stumble to stop",
        "para3Item2Title2": "❌ Lose balance and kick the box",
        "para4Subtitle1": "Our Success Cases",
        "para4Subtitle2": "Failure Cases of Baseline Methods",
        "para4Item1Title1": "WholeBodyVLA (ours)",
        "para4Item1Title2": "WholeBodyVLA under unseen object",
        "para4Item2Title1": "❌ Stumble to stop",
        "para4Item2Title2": "❌ Lose balance and deviate greatly from the intended direction",
        "para5Subtitle1": "Our Success Cases",
        "para5Subtitle2": "Failure Cases of Baseline Methods",
        "para5Item1Title1": "WholeBodyVLA (ours)",
        "para5Item1Title2": "WholeBodyVLA under unseen heavy load",
        "para5Item2Title1": "❌ Deviate from the right direction",
        "para5Item2Title2": "❌ Stop too late",
        "para6Subtitle1": "1. Object Generalization",
        "para6Subtitle2": "2. Start-Pose Generalization",
        "para6Subtitle3": "3. Terrian Generalization",
        "para6SubArtical1": "Demonstrate WholeBodyVLA's robustness to variations in objects appearance and position, layout, and table color.",
        "para6SubArtical2": "Showcase WholeBodyVLA's ability to compose forward advancing, sidestepping, turning, and squatting to handle diverse start-poses (X/Y offsets, orientations, and table heights).",
        "para6SubArtical3": "Demonstrate WholeBodyVLA's ability to traverse uneven terrain.",
        "para6Subtitle1s": "Distance X-Axis",
        "para6Subtitle2s": "Distance Y-Axis",
        "para6Subtitle3s": "Orientation",
        "para6Subtitle4s": "Height",
        "para6Item1Title1": "X-axis Distance Generalization Experiment 1",
        "para6Item1Title2": "X-axis Distance Generalization Experiment 2 (w/ unseen table color)",
        "para6Item2Title1": "Y-axis Distance Generalization Experiment 1",
        "para6Item2Title2": "Y-axis Distance Generalization Experiment 2 (w/ unseen table color)",
        "para6Item3Title1": "Orientation Generalization Experiment 1",
        "para6Item3Title2": "Orientation Generalization Experiment 2 (w/ unseen table color)",
        "para6Item4Title1": "Height Generalization Experiment",
        "para7Artical": "Showcase WholeBodyVLA's visual navigation and obstacle-avoidance capabilities over long trajectories.",
        "para7ItemTitle1": "Visual Navigation, Sign Following",
        "para7ItemTitle2": "Obstacle Avoidance",
        "para8Artical": "Demonstrate WholeBodyVLA's competence on long-horizon sequences that involve loco-manipualtion and whole-body coordinated actions.",
        "para8ItemTitle1": "Long-Horizon Bimanual Manipulation with Coordination",
        "para9Artical": "Showcase WholeBodyVLA's scalability to more complex everyday loco-manipulation tasks (e.g., wiping, vacuum cleaning, etc)."
    }
}
