{
    "Layout": {
        "home": "首页",
        "careers": "招聘",
        "about": "关于",
        "product": "KaiBot",
        "research": "研究",
        "email": "电子邮箱",
        "subscribe": "订阅",
        "subscribed": "已订阅",
        "footerRightTit": "订阅超维动力",
        "footerRightP": "第一时间获取超维动力的最新资讯",
        "footerInputTxt": "请输入邮箱地址",
        "footerCopyright": "Copyright © 2025. 超维动力 All rights reserved. "

    },
    "Home": {
        "title": "超维动力",
        "indexBannerTit": "打造超高拟人机器人<p>智能涌现、运动涌现、情感涌现</p>",
        "indexCompanyTit": "<span>公司</span>简介​",
        "indexCompanyPara": "<p>超维动力，是一群对技术创新狂热的极客打造的前沿科技公司，致力于研发最极致拟人的人形机器人。我们独创“物理世界模型+全身”驱动系统，让机器人成为兼具理性决策与情感共鸣的物理智能体。通过在虚实贯通的人类环境中持久互动，我们正在不断推动通用智能的突破，迈向全场景、全维度智能应用的新时代。</p>",
        "indexCompanyBtn": "了解更多",
        "indexTechTit": "核心<span>技术</span>",
        "indexTechSTit": "聪明、易用、好用的高拟人人形机器人",
        "indexSceneTit": "核心<span>场景</span>",
        "indexNewsTit": "<span>新闻</span>中心",
        "indexNewsBtn": "了解更多新闻"
    },
    "Careers": {
        "allpositions": "全部职位",
        "filterCriteria": "筛选条件",
        "emptyTxt": "没有找到符合条件的职位！",
        "departmentsTit": "部门",
        "locationsTit": "地点",
        "workTypesTit": "工作类型",
        "allDepartments": "所有部门",
        "allLocations": "所有地点",
        "allWorkTypes": "所有工作类型",
        "loading": "加载中"
    },
    "Company": {
        "bannerTit": "关于我们",
        "introTit": "企业介绍",
        "introPara": "<p>超维动力，是一群对技术创新狂热的极客打造的前沿科技公司，致力于研发最极致拟人的人形机器人。我们独创“物理世界模型+全身”驱动系统，让机器人成为兼具理性决策与情感共鸣的物理智能体。通过在虚实结合的人类环境中持久互动，我们正在不断推动通用智能的突破，迈向全场景、全维度智能应用的新时代。</p><p>核心团队来自于机器人、自动驾驶和人工智能领域，凭借深厚的产品工程化能力和从零到一落地前沿科技的完整实力，不断引领智能科技的创新之路。我们汇聚了十多位世界级人工智能算法科学家，谷歌学术引用十余万，彰显了我们在算法研发领域全球领先的实力。</p>",
        "teamTit": "团队故事"
    },
    "Product": {
        "txt": "将于2026年初到来。敬请期待。"
    },
    "Research": {
        "label": "研究",
        "para": "<p>探索具身智能落地所需的每一个技术和工程细节</p><p>真正打造超拟人人形机器人</p>"
    },
    "ResearchDetail1": {
        "label": "Research",
        "titleTips": "AMS",
        "title": "Agility Meets Stability: Versatile Humanoid Control with Heterogeneous Data",
        "share1": "arXiv",
        "share2": "YouTube",
        "share3": "Code (Est. Feb 2026) ",
        "copy": "Share",
        "date": "November 24, 2025",
        "copysuccess": "The page link has been copied",
        "paraTitle1": "Extreme Balance Motions",
        "paraTitle2": "Dynamic Motions",
        "paraTitle3": "IMU-based Teleoperation",
        "paraTitle4": "RGB-based Real-time Teleoperation",
        "paraTitle5": "Abstract",
        "paraTitle6": "Overview of AMS",
        "paraTitle7": "Citation",
        "para1Subtitle1": "Ip Man's Squat (Unseen Data) - (叶问蹲)",
        "para1Subtitle2": "Single-Leg Balance Standing (Randomly Generated)",
        "para2ItemTitle1": "Running",
        "para2ItemTitle2": "Basketball Dribbling",
        "para2ItemTitle3": "Waist Twists (Unseen Data from Video)",
        "para2ItemTitle4": "Lunge (Unseen Data from Video)",
        "para2ItemTitle5": "Swing and Squat (Unseen Data from Video)",
        "para2ItemTitle6": "Prayer Squat (Unseen Data from Video)",
        "para3Subtitle1": "(No Optical MoCap System in Use, 1x)",
        "para3Item1Title1": "Kungfu",
        "para3Item1Title2": "Football",
        "para3Subtitle2": "Multi-robots Teleoperation",
        "para5Artical": "<p>Humanoid robots are envisioned to perform a wide range of tasks in human-centered environments, requiring controllers that combine agility with robust balance. Recent advances in locomotion and whole-body tracking have enabled impressive progress in either agile dynamic skills or stability-critical behaviors, but existing methods remain specialized, focusing on one capability while compromising the other.</p><p>In this work, we introduce AMS (Agility Meets Stability), the first framework that unifies both dynamic motion tracking and extreme balance maintenance in a single policy. Our key insight is to leverage heterogeneous data sources: human motion capture datasets that provide rich, agile behaviors, and physically constrained synthetic balance motions that capture stability configurations. To reconcile the divergent optimization goals of agility and stability, we design a hybrid reward scheme that applies general tracking objectives across all data while injecting balance-specific priors only into synthetic motions. Further, an adaptive learning strategy with performance-driven sampling and motion-specific reward shaping enables efficient training across diverse motion distributions.</p><p>We validate AMS extensively in simulation and on a real Unitree G1 humanoid. Experiments demonstrate that a single policy can execute agile skills such as dancing and running, while also performing zero-shot extreme balance motions like Ip Man's Squat, highlighting AMS as a versatile control paradigm for future humanoid applications.</p>",
        "para6Artical": "<p>(a) The general whole-body tracking pipeline retargets human MoCap data to reference motions and adopts a teacher-student-based strategy for reinforcement learning To address data limitations and conflicting optimization objectives, AMS introduces three key components as follows. (b) Synthetic balance data is generated to complement human MoCap data and address data limitations. (c) Adaptive learning is employed with adaptive sampling and reward shaping based on individual motion performance. (d) Hybrid rewards are designed with general rewards for all motions and balance prior rewards exclusively for synthetic motions.</p>"
    },
    "ResearchDetail2": {
        "label": "Research",
        "titleTips": "WholeBodyVLA",
        "title": "Towards Unified Latent VLA for Whole-body Loco-manipulation Control",
        "share1": "Video",
        "share2": "arXiv",
        "share4": "GitHub",
        "date": "November 24, 2025",
        "paraTitle1": "Overview of WholeBodyVLA",
        "paraTitle2": "Method Overview",
        "paraTitle3": "Task 1: Bag Packing",
        "paraTitle4": "Task 2: Box Loading",
        "paraTitle5": "Task 3: Cart Pushing",
        "paraTitle6": "Generalization Experiments",
        "paraTitle7": "Navigation Capabilities",
        "paraTitle8": "Long-Horizon Bimanual Manipulation",
        "paraTitle9": "What's More",
        "para1Artical": "Introducing WholeBodyVLA, a humanoid system that operates on Agibot X2 robot and performs end-to-end humanoid loco–manipulation in large space for the first time. The proposed system achieves consecutive tasks autonomously, including (a-c) basic bimanual grasping, side-step toward the box, and squatting to place; (d-e) squatting to grasp and lift the box and turning to place the box onto the cart; (f-h) grasping the cart handle, pushing the cart forward, and pushing a load of more than 50 kg.",
        "para2Artical": "<span>Pipeline of WholeBodyVLA.</span> LAM is pretrained on manipulation and manipulation- aware locomotion videos, yielding unified latent supervision for the VLM. Meanwhile, the LMO RL policy is trained for precise and stable locomotion under disturbances. At runtime, egocentric images and language instructions are encoded by the VLM into latent action tokens, which are decoded (∼10 Hz) into (i) dual-arm joint actions and (ii) locomotion commands executed by LMO at 50 Hz, enabling robust whole-body loco–manipulation.",
        "para3Subtitle1": "Our Success Cases",
        "para3Subtitle2": "Failure Cases of Baseline Methods",
        "para3Item1Title1": "WholeBodyVLA (ours)",
        "para3Item1Title2": "WholeBodyVLA under visual variation",
        "para3Item2Title1": "❌ Stumble to stop",
        "para3Item2Title2": "❌ Lose balance and kick the box",
        "para4Subtitle1": "Our Success Cases",
        "para4Subtitle2": "Failure Cases of Baseline Methods",
        "para4Item1Title1": "WholeBodyVLA (ours)",
        "para4Item1Title2": "WholeBodyVLA under unseen object",
        "para4Item2Title1": "❌ Stumble to stop",
        "para4Item2Title2": "❌ Lose balance and deviate greatly from the intended direction",
        "para5Subtitle1": "Our Success Cases",
        "para5Subtitle2": "Failure Cases of Baseline Methods",
        "para5Item1Title1": "WholeBodyVLA (ours)",
        "para5Item1Title2": "WholeBodyVLA under unseen heavy load",
        "para5Item2Title1": "❌ Deviate from the right direction",
        "para5Item2Title2": "❌ Stop too late",
        "para6Subtitle1": "1. Object Generalization",
        "para6Subtitle2": "2. Start-Pose Generalization",
        "para6Subtitle3": "3. Terrian Generalization",
        "para6SubArtical1": "Demonstrate WholeBodyVLA's robustness to variations in objects appearance and position, layout, and table color.",
        "para6SubArtical2": "Showcase WholeBodyVLA's ability to compose forward advancing, sidestepping, turning, and squatting to handle diverse start-poses (X/Y offsets, orientations, and table heights).",
        "para6SubArtical3": "Demonstrate WholeBodyVLA's ability to traverse uneven terrain.",
        "para6Subtitle1s": "Distance X-Axis",
        "para6Subtitle2s": "Distance Y-Axis",
        "para6Subtitle3s": "Orientation",
        "para6Subtitle4s": "Height",
        "para6Item1Title1": "X-axis Distance Generalization Experiment 1",
        "para6Item1Title2": "X-axis Distance Generalization Experiment 2 (w/ unseen table color)",
        "para6Item2Title1": "Y-axis Distance Generalization Experiment 1",
        "para6Item2Title2": "Y-axis Distance Generalization Experiment 2 (w/ unseen table color)",
        "para6Item3Title1": "Orientation Generalization Experiment 1",
        "para6Item3Title2": "Orientation Generalization Experiment 2 (w/ unseen table color)",
        "para6Item4Title1": "Height Generalization Experiment",
        "para7Artical": "Showcase WholeBodyVLA's visual navigation and obstacle-avoidance capabilities over long trajectories.",
        "para7ItemTitle1": "Visual Navigation, Sign Following",
        "para7ItemTitle2": "Obstacle Avoidance",
        "para8Artical": "Demonstrate WholeBodyVLA's competence on long-horizon sequences that involve loco-manipualtion and whole-body coordinated actions.",
        "para8ItemTitle1": "Long-Horizon Bimanual Manipulation with Coordination",
        "para9Artical": "Showcase WholeBodyVLA's scalability to more complex everyday loco-manipulation tasks (e.g., wiping, vacuum cleaning, etc)."
    }
}
